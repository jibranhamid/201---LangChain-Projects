{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNxBK/YkmKxZY02DPA48vQt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jibranhamid/201---LangChain-Projects/blob/main/HelloWorld_LangChain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-b3IJ7LuTmI5",
        "outputId": "d8033deb-b9a3-4cfe-afd6-312032aef2e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.0 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/411.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.6/411.6 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -Uq langchain langchain-google-genai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from google.colab import userdata\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rThrs5nvT8qS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm: ChatGoogleGenerativeAI = ChatGoogleGenerativeAI(\n",
        "    model = 'gemini-2.0-flash-exp',\n",
        "    api_key = userdata.get('GOOGLE_API_KEY'),\n",
        "    temperature=0.9\n",
        ")\n",
        "\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"You are a helpful assistant. Answer the following question:\\n\\n{question}\"\n",
        ")\n",
        "\n",
        "chain = prompt_template | llm\n",
        "\n",
        "\n",
        "question = \"What is LangChain?\"\n",
        "response = chain.invoke({\"question\": question})\n",
        "\n",
        "answer = response.content.strip()\n",
        "\n",
        "formatted_answer = f\"### Answer:\\n\\n{answer}\"\n",
        "\n",
        "\n",
        "display(Markdown(formatted_answer))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 755
        },
        "id": "8n7RBCB2UU2T",
        "outputId": "fd300e08-b759-4a70-ba1d-184154906000"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Answer:\n\nLangChain is a framework designed to simplify the process of building applications using large language models (LLMs). Think of it as a toolkit that provides a set of components, utilities, and abstractions that make it easier to:\n\n* **Connect to various LLMs:** LangChain supports a wide range of LLMs, including OpenAI's GPT models, Hugging Face's transformers, and many others. This allows you to easily switch between models or experiment with different options.\n* **Manage and prompt LLMs:** It offers tools for crafting effective prompts, managing conversation history, and controlling the LLM's behavior.\n* **Chain together different components:** LangChain allows you to create complex workflows by linking together different LLMs, tools, and data sources. This is where the \"chain\" in LangChain comes from, enabling you to build more sophisticated applications than just simple prompt-response interactions.\n* **Integrate with external data sources:** LangChain can connect to various data sources, such as APIs, databases, and document stores. This enables you to build applications that use real-world data and information.\n* **Build agents:** LangChain provides abstractions for creating AI agents that can take actions and interact with the world. This allows you to build applications that go beyond simple question-answering, such as creating tools that can search the web, write emails, or schedule appointments.\n\n**In essence, LangChain provides the infrastructure and building blocks to:**\n\n* **Democratize access to LLMs:** Makes it easier for developers, even those without deep machine learning expertise, to leverage the power of LLMs.\n* **Accelerate development:** Reduces the boilerplate code needed to build complex LLM applications.\n* **Increase flexibility and adaptability:** Allows you to easily experiment with different LLMs, data sources, and tools.\n* **Enable more powerful and intelligent applications:** Facilitates the creation of applications that go beyond simple text generation.\n\n**Key Features and Concepts in LangChain:**\n\n* **LLMs:** The core of LangChain, representing the various language models it supports.\n* **Prompts:** Templates and instructions used to guide the LLM.\n* **Chains:** Sequences of LLM calls and other operations.\n* **Indexes:** Structures for storing and retrieving data for use with LLMs.\n* **Agents:** Autonomous entities that can make decisions and take actions.\n* **Memory:** Mechanisms for maintaining conversation history and context.\n* **Tools:** External functions that agents can use to interact with the world.\n\n**Think of it like this:**\n\nImagine you want to build a chatbot that can answer questions about your company's documentation. Without LangChain, you would have to:\n\n1.  Figure out how to connect to a language model.\n2.  Write code to format the user's question into a suitable prompt.\n3.  Implement logic to find the relevant documents from your company's data.\n4.  Pass the documents to the language model for summarization and answer generation.\n5.  Handle conversation history and context.\n\nWith LangChain, you can leverage its components to simplify these steps, allowing you to focus on the logic of your specific application.\n\n**In Summary:**\n\nLangChain is a powerful and versatile framework that empowers developers to build sophisticated applications powered by large language models, making them easier to access, manage, and integrate into complex workflows. It's a key tool for anyone looking to harness the full potential of LLMs."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ycdcQzZEYXJe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}